\chapter{Spazio Probabilizzato}

\section{Lo Spazio Probabilizzato}


\begin{defn}
	L'insieme delle parti di A, indicato con $ \parts{A}$, \`e dato da tutti i sottoinsiemi che posso costruire a partire dagli elementi di A. Ad esempio:

	\begin{equation*}
		\begin{aligned}
			A = \{0, 1\} \\
			\parts{A} = \{\emptyset, \{0, 1\}, \{0\}, \{1\}\}
		\end{aligned}
	\end{equation*}
\end{defn}

\begin{defn}
	Un inseme	$ F \subseteq \parts{A} $ chiuso rispetto a intersezione, unione e complementare, ovvero

	\begin{equation*}
	\begin{aligned}
		A,B \in F \implies \begin{cases}
			A \cup B \\
			A \cap B \\
			A^C, B^C
		\end{cases} \in F
	\end{aligned}
	\end{equation*}
	Si chiama {\em algebra}.

	Se \`e chiuso rispetto all'unione numerabile di insiemi, ovvero se $ A_1, \dots, A_n \subset F $ allora $ \bigcup\limits_{n \in \N} A_n \in F $, allora $ F $  si dice  $ \sigma $-Algebra (o trib\`u).
\end{defn}
Uno \textbf{spazio probabilizzato} \`e un ente matematico che serve per introdurre una definizione pi\`u rigorosa e pi\`u flessibile di probabilit\`a rispetto a quella usata fino ad adesso.
\begin{defn}
	 Uno spazio probabilizzato \`e definito come una terna:

	\begin{equation}
		(\Omega, F, P)
	\end{equation}

	Dove $ \Omega $ \`e l'insieme degli eventi elementari, ovvero tutti i risultati possibili, ad esempio in un lancio di un dado $ \Omega = \{1,2,3,4,5,6\} $, $F$ \`e una $\sigma$-algebra contenuta nelle parti di $\Omega$ e 
	la probabilit\`a $ P $ \`e una funzione definita come

	\begin{eqnarray}
			&P : F \to [0,1] \nonumber\\
			&P(\Omega) = 1 \\
			&\forall i \neq j \> . \> A_i \cap A_j \neq \emptyset \implies P \left( \bigcup\limits_{n} A_n \right) = \sum_{n} P(A_n)\nonumber
	\end{eqnarray}
\end{defn}

Spesso, se $ \Omega $ \`e un insieme finito, allora si prende $ F = \parts{\Omega} $ e
	\begin{equation*}
		P (A \subset F) = \dfrac{\#A}{\#\Omega}
	\end{equation*}
	ritrovando il concetto intuitivo del primo capitolo, ma definito in maniera rigorosa.

	\begin{prop}
		Dagli assiomi di probabilit\`a dati sopra si possono dimostrare le seguenti propriet\`a:
		
		\begin{equation}
			\begin{aligned}
				P(A^C) = 1 - P(A) 	\\
				P(A \cup B) = P(A) + P(B) - P(A \cap B) \\
				A \subseteq B \implies P(A) \leq P(B)
			\end{aligned}
		\end{equation}
	\end{prop}


\begin{exmp}
	\textbf{Lotteria di De' Finetti:}
	Si assiste all'estrazione di un numero $ n \in \N $ casuale. Supponiamo che ogni numero abbia la stessa probabilit\`a di essere estratto.
	
	Dato un altro naturale $ m \in \N $
	
	\begin{equation*}
	p_n = P(n) = \text{ La probabilit\`a di estrarre il numero n}
	\end{equation*}
	
	Vogliamo che  $ 0 \leq p_n \leq 1 $ e anche $ p_m = p_n$ $\forall n = m $. Quindi $ 1 = P(\Omega) = P \left( \bigcup\limits_{n \in \N} n \right) = \sum_{n \in \N} p_n $. Assumendo $ p_n = 0$ $\forall n $ allora $ \sum_{n} p_n = 0 $. Se $ p_n = c > 0, \forall n $ allora $ \sum_{n} p_n = \sum_{n} c = + \infty $
	In nessuno dei due casi \`e possibile che $ \p{\Omega} = \sum_{n \in \N} p_n = 1$, quindi la lotteria di De Finetti non si pu\`o modellizzare con l'ipotesi che ogni numero sia equiprobabile.

	Si noti che nello stesso esempio, se si prende come $F=\{\emptyset, \{\text{numeri pari}\},\{\text{numeri dispari}\},\mathbb{N}\}$, supponendo sempre che ogni numero abbia la stessa probabilit\`a di venire estratto, si pu\`o dare una buona definizione di probabilit\`a almeno all'estrazione di un numero pari o dispari:
	\begin{eqnarray*}
	&P(\emptyset)=0\\
	&P(\{\text{numeri pari}\})=1/2\\
	&P(\{\text{numeri dispari}\})=1/2\\
	&P(\mathbb{N})=1/2
	\end{eqnarray*}
Questo \`e un esempio in cui \`e utile prendere una $\sigma$-algebra che non coincida con le parti di $A$
\end{exmp}


\begin{defn}
	\textbf{Densit\`a di Probabilit\`a discreta:}
	Una successione $ \{p_n\}_{n \in \N} $ con $ p_n \in \R $ con $ p_n \geq 0 $ e $ \sum_{n \in \N} p_n = 1 $
	\`e detta densit\`a di probabilit\`a discreta.

	Se $ \{p_n\} $ \`e una densit\`a di probabilit\`a discreta, allora su $ ( \N, \parts{\N}) $ si pu\`o definire la probabilit\`a $P(n)=p_n$. In tal modo $ ( \N, \parts{\N},P) $ diventa uno spazio probabilizzato.
\end{defn}

\begin{defn}
	\textbf{Ultimo assioma di probabilit\`a:}
	Se $P(B)\neq0$, si definisce la probabilit\`a condizionata come $$ P(A \mid B) := \dfrac{P(A \cap B)}{P(B)} .$$
	e si dice che $A$ e $B$ sono indipendenti se  $ P(A \mid B) = P(A) $.
\end{defn}

Questo ultimo assioma rende rigoroso quello che avevamo visto con la probabilit\`a intuitiva nel capitolo precedente. La definizione di indipendenza a prima vista sembra strana: da una parte il ruolo di $A$ e $B$
sembra simmetrico (si dicono indipendenti entrambi), ma nella formula il ruolo di $A$ e $B$ non sembra
intercambiabile. In effetti anche nella formula precedente i ruoli si possono scambiare, come dimostra la 
proposizione seguente.
\begin{prop}
	Se $ P(A \mid B) = P(A)$ allora $P(B \mid A) = P(B) $
\end{prop}
\begin{proof} Supponiamo che entrambi gli eventi non siano impossibili. Si ha
	\begin{equation*}
	\begin{aligned}
		P(A \mid B) = \dfrac{P(A \cap B)}{P(B)} \\
		P(B \mid A) = \dfrac{P(B \cap A)}{P(A)}
	\end{aligned}
	\end{equation*}
Allora
$$
P(A)=P(A \mid B) = \dfrac{P(A \cap B)}{P(B)}= \dfrac{P(B \cap A)}{P(B)}
$$
da cui segue 
$$
P(B)= \dfrac{P(B \cap A)}{P(A)}=P(B \mid A).
$$
\end{proof}	
Si pu\`o usare la definizione equivalente di eventi indipendenti
\begin{defn}
$A$ e $B$ sono indipendenti se $\dfrac{P(A \cap B)}{P(B)}=P(A)\cdot P(B)$
\end{defn}
Quest'ultima definizione si generalizza bene al caso di molti eventi indipendenti.
\begin{defn}
Gli eventi $ A_1, \dots, A_n $ si dicono indipendenti se per ogni scelta di indici $i_1, \dots, i_k $ vale
	\begin{equation*}
	 P \left(A_{i_1}\cap\dots\cap A_{i_k}\right) = P(A_{i_1}) \cdot \dots \cdot P(A_{i_k})
	\end{equation*}
\end{defn}

\begin{exmp}
	$ A, B $ indipendenti $ \implies \left\{(A, B^C), (A^C, B^C), (A^C, B) \right\}$ indipendenti
	
	\begin{proof} Dimostriamo solo che $ A $ e $ B^C $ sono indipendenti, le altre sono analoghe
		\begin{equation*}
			\begin{aligned}
				A = (A \cap B) \cup (A \cap B^C)\implies  \\
				P(A) = P(A \cap B) + P(A \cap B^C) = P(A) \cdot P(B) + P(A \cap B^C) \implies\\
				P(A \cap B^C) = P(A) - P(A) \cdot P(B) =
				P(A)\left[1 - P(B)\right] = P(A) \cdot P(B^C)
			\end{aligned}
		\end{equation*}

	\end{proof}
\end{exmp}


\begin{exrc}
	Lancio due dadi, uno rosso ed uno nero. Definiamo lo spazio probabilizzato con
	$ \Omega = \{(r, n), \text{ dove } r = 1,2,3,4,5,6;\  n = 1,2,3,4,5,6 \}$,
	$ F = \parts{\Omega}$, e la probabilit\`a intuitiva, es $ \prob{n=1} = \dfrac{1}{36} $.

	Calcoliamo la probabilit\`a che il rosso sia 3 sapendo che rosso + nero fa 6
	\begin{equation*}
	\prob{r=3  \mid  r+n = 6} = \dfrac{\prob{r=3 \cap r + n = 6}}{\prob{r+n=6}} = \frac{\frac{1}{36}}{\frac{5}{36}} = \frac{1}{5}
	\end{equation*}
	
	Probabilit\`a che il rosso sia pari sapendo che rosso + nero fa 6

	\begin{equation*}
	\prob{r = \text{ pari}  \mid  r+n = 6} = \dfrac{\prob{r \text{ pari} \cap r+n = 6}}{\prob{r+n = 6}} = \frac{\frac{2}{36}}{\frac{5}{36}} = \dfrac{2}{5}
	\end{equation*}
\end{exrc}


\begin{exrc}
	\textbf{Gioco di Monty Hall:}
	Riprendendo il gioco delle tre porte definiamo lo spazio probabilizzato: Formalizzo di aver scelto la porta 3. $ \Omega = (x,y) $ dove $ x = 1,2,3 $ \`e la porta vincente e $ y = 1,2 $ \`e la porta perdente che \`e stata aperta dal presentatore. Gli eventi impossibili saranno $ \prob{1,1} = 0, \prob{2,2} = 0 $
	
	\begin{equation*}
	\begin{aligned}
	\prob{x=1} = \prob{x=2} = \prob{x=3} = \dfrac{1}{3} \\
	\prob{x=1,y=2} = \dfrac{1}{3} \\
	\prob{x=2,y=1} = \dfrac{1}{3} \\
	\prob{x=3,y=1} = \prob{x=3,y=2} = \dfrac{1}{6} 
	\end{aligned}
	\end{equation*}
\end{exrc}





Quindi $ \prob{y=1} = \prob{y=2} = \dfrac{1}{2} $

Se scelgo la porta 3, suppongo venga aperta la 2. Se non cambio e vinco $ (x = 3) $ allora

\begin{equation*}
	\prob{x=3  \mid  y=2} = \dfrac{\prob{(3,2)}}{\prob{y=2}} = \dfrac{\dfrac{1}{6}}{\dfrac{1}{2}} = \dfrac{1}{3}
\end{equation*}


Se scelgo la porta 3, suppongo venga aperta la 1 e cambio allora:

\begin{equation}
	\prob{x=1  \mid  y=2} = \dfrac{\prob{(1,2)}}{\prob{y=2}} = \dfrac{\dfrac{1}{3}}{\dfrac{1}{2}} = \dfrac{2}{3}
\end{equation}

\section{Formula di fattorizzazione}

\begin{defn}
	Dato uno spazio probabilizzabile $(\Omega, F)$,
	una famiglia di insiemi $ B_1, \dots, B_n \in F$, con $ n \in \N $  \`e detta una partizione finita di $ \Omega $ se  se $ \forall i,j$ con  $j\neq i$ allora  $B_i \cap B_j = \emptyset $ e se
	$ \bigcup_{i=1}^{n} B_i = \Omega $.
	\end{defn}

\begin{lem}
	Sia $ \{B_i\}_{i=1, \dots, n} $ partizione finita di $ \Omega $ e sia $ \prob{B_i} > 0 $ $\forall i$. Allora si avr\`a


	\begin{equation}
		\prob{A} = \sum_{i=1}^{n} \prob{A \mid B_i} \cdot \prob{B_i}
	\end{equation}
	
	\begin{proof}
		\begin{equation*}
			\prob{A} =  \sum_{i=1}^{n} \prob{A \cap B_i} = \sum_{i=1}^{n} \prob{A \mid B_i} \cdot \prob{B_i}
		\end{equation*}
	\end{proof}
\end{lem}

\begin{defn}
	\textbf{Condizionamento Ripetuto:}
	Dati $ A_1, \dots, A_n $ eventi, allora 
	\begin{equation*}
		\prob{A_1 \cap \dots \cap A_n} = \prob{A_1} \cdot \prob{A_2 \mid A_1} \cdot \prob{A_3 \mid  A_1 \cap A_2} \cdot\, \dots\, \cdot \prob{A_n  \mid  A_1 \cap \dots \cap A_{n-1}}
	\end{equation*}
\end{defn}

\section{Formula di Bayes}

\begin{lem}
	Dati due eventi $ A,B $ con probabilit\`a non nulla $ \prob{A} > 0 $ e $ \prob{B} > 0 $ allora 
	
	\begin{equation}
	\prob{A \mid B} = \dfrac{\p{B \mid A} \cdot \p{A} }{\p{B}}
	\end{equation}
	
	\begin{proof}
		\begin{eqnarray*}
			\p{A \mid B} &=& \frac{\p{A\cap B}}{\p{B}} \\
			\p{B \mid A} &=& \frac{\p{B\cap A}}{\p{A}}=\frac{\p{A\cap B}}{\p{A}} \\
			\implies \p{A\cap B} &=&  \p{A\mid B}\cdot \p{B}=\p{B\mid A}\cdot \p{A}, \\
			\implies \p{A \mid B} &=&  \frac{\p{B \mid A}  \cdot \p{A}} {\p{B}}.
		\end{eqnarray*}
	\end{proof}
\end{lem}

\section{Esercizi}


\begin{exrc}
	Un produttore di vino produce due vini (bianco $ B $ e rosso $ R $) e vende in Francia ($ F $) e Germania ($ G $). 
	Le vendite totali sono $ 1/3 $ per la Francia e $ 2/3 $ per la Germania. Le richieste della Francia sono
	per $ 3/4 $ vino bianco e per $ 1/4 $ di vino rosso. Le richieste della Germania si dividono equamente
	tra vino bianco e vino rosso
	Utilizzando la formula di partizione troviamo la probabilit\`a che una richiesta (senza sapere
	da chi viene fatta) sia vino bianco.

	\begin{equation*}
	\p{B} = \p{B \mid G} \cdot \p{G} + \p{B \mid F} \cdot \p{F} = \frac12 \cdot \frac23 +
	\frac34 \cdot \frac13= \dfrac{7}{12}
	\end{equation*}
\end{exrc}

\begin{exrc}
	Abbiamo 3 livelli di preparazione di degli studenti iscritti ad un esame: Ottimo, Buono e Scarso.
	Un esito dell'esame \`e Promosso o Respinto.
	
	\begin{equation*}
		\begin{aligned}
		\p{\text{Promosso}  \mid  \text{Ottimo}} = 0.995\\
		\p{\text{Promosso}  \mid  \text{Scarso}} = 0.3\\
		\p{\text{Promosso}  \mid  \text{Buono}} = 0.8\\
		\end{aligned}
	\end{equation*}
	
	Uno studente prova l'esame e viene respinto. Qual \`e la probabilit\`a che avesse una preparazione scarsa (ovvero $ \p{\text{Scarso} \mid \text{Respinto}} $)? Prima calcoliamo la probabilit\`a di essere respinti.
	
	\begin{equation*}
	\p{R} = \p{R \mid O} \cdot \p{O} + \p{R  \mid  B} \cdot \p{B} + \p{R \mid S} \cdot \p{S} = 0.302
	\end{equation*}
	
	Senza informazioni aggiuntive $ \p{O} = \p{B} = \p{S} = \dfrac{1}{3} $. 
	La probabilit\`a di essere respinto \`e $ \p{R} = 0.302 $ quindi 
	
	\begin{equation*}
	\p{S  \mid  R} = \dfrac{\p{R \mid S}\cdot \p{S}}{\p{R}} = \dfrac{0.7 \cdot 1/3}{0.302} = 0.773
	\end{equation*}
\end{exrc}

\begin{exrc}
	Qual \`e la probabilit\`a che lo studente aveva di avere una preparazione scarsa, sapendo che \`e stato respinto e sapendo che le probabilit\`a dei voti sono:

	\begin{equation*}
		 \p{O} = \dfrac{1}{6}, \p{B} = \dfrac{2}{3}, \p{S} = \dfrac{1}{6}
	\end{equation*}
	
	Calcoliamo, come prima $ \p{R} = 0.005 \cdot \dfrac{1}{6} + 0.2 \cdot \dfrac{2}{3} + 0.7 \cdot \dfrac{1}{6} \approx 0.25 $. Abbiamo quindi che 
	
	\begin{equation*}
		\p{S \mid R} = \dfrac{0.7 \cdot 1/6}{0.25} \approx 0.466
	\end{equation*}
\end{exrc}

\begin{exrc}
	La probabilit\`a di ammalarsi di un soggetto a rischio $ (R) $ \`e $ 0.2 $, mentre la probabilit\`a di ammalarsi di un soggetto non a rischio $ (N) $ \`e $ 0.006 $. Il $ 15\% $ della popolazione \`e di soggetti a rischio. Un malato si denota con $ M $ mentre uno sano con $ S $. Vogliamo sapere

	\begin{enumerate}
		\item $ \p{\text{Soggetto casuale sia malato}} = $
		
		$ \p{M} = \p{M \mid R} \cdot \p{R} + \p{R \mid N} \cdot \p{N} = 0.35 $ 
		
		$ \p{M} = 0.2 \cdot 0.15 + 0.006 \cdot 0.85 = 0.35 $
		
		\item $ \p{\text{Soggetto malato fosse a rischio}} = $
		
		$ \p{R \mid M} = \dfrac{\p{M \mid R} \cdot \p{R}}{\p{M}} = \dfrac{0.2 \cdot 0.15}{0.35} = 0.855 $
		
		\item $ \p{\text{Soggetto soggetto sano sia a rischio}} = $
		
		$ \p{R \mid S} = \dfrac{\p{S \mid R} \cdot \p{R}}{\p{S}} = \dfrac{(1-0.2) \cdot 0.15}{(1-0.35)} = 0.124$
	\end{enumerate}
	
	\begin{note}
		La probabilit\`a che l'evento $ A^c $ ($ A $ complementare) si verifichi sapendo B \`e $ \p{A^c \mid B} = 1 - \p{A \mid B} $ 
		mentre la probabilit\`a di $ A $ sapendo $ B^c $ \`e $ \p{A \mid B^c} \not\equiv 1 - \p{A \mid B} $.
	\end{note}

	\begin{defn}
		Prendendo $ S = $ soggetti sani, $ M = $ soggetti malati, $ T^- = $ test negativo, $ T^+ = $ test positivo. 
		La \textbf{specificit\`a} di un test \`e $ \p{T^- \mid S} $. Una specificit\`a alta implica pochi falsi positivi. 
		La \textbf{sensibilit\`a} \`e $ \p{T^+ \mid M} $. Una sensibilit\`a alta implica pochi falsi negativi.
	\end{defn}
\end{exrc}


